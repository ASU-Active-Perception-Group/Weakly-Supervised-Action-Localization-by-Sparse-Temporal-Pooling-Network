{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import utils.utils as util\n",
    "import time\n",
    "import os\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, args):\n",
    "        self.trainidx = []\n",
    "        self.dataset_name = args.dataset_name\n",
    "        self.path_to_annotations = os.path.join(args.annotation_path, args.dataset_name + '-Annotations/')\n",
    "        self.path_to_features = os.path.join(args.I3D_path, self.dataset_name + '-I3D-JOINTFeatures.npy')\n",
    "        self.labels = np.load(self.path_to_annotations + 'labels_all.npy')     # Specific to Thumos14\n",
    "        self.classlist = np.load(self.path_to_annotations + 'classlist.npy')\n",
    "        self.subset = np.load(self.path_to_annotations + 'subset.npy')\n",
    "        self.testidx = []\n",
    "        self.classwiseidx = []\n",
    "        self.train_test_idx()\n",
    "        self.currenttestidx = 0\n",
    "        self.t_max = args.max_seqlen\n",
    "        self.num_class = args.num_class\n",
    "        self.classwise_feature_mapping()\n",
    "        self.batch_size = args.batch_size\n",
    "        self.feature_size = args.feature_size\n",
    "        self.features = np.load(self.path_to_features, encoding='bytes')\n",
    "        self.segments = np.load(self.path_to_annotations + 'segments.npy')\n",
    "        self.labels_multihot = [util.strlist2multihot(labs,self.classlist) for labs in self.labels]\n",
    "\n",
    "\n",
    "    def train_test_idx(self):\n",
    "        for i, s in enumerate(self.subset):\n",
    "            if s.decode('utf-8') == 'validation':   # Specific to Thumos14\n",
    "                self.trainidx.append(i)\n",
    "            else:\n",
    "                self.testidx.append(i)\n",
    "\n",
    "    def classwise_feature_mapping(self):\n",
    "        for category in self.classlist:\n",
    "            idx = []\n",
    "            for i in self.trainidx:\n",
    "                for label in self.labels[i]:\n",
    "                    if label == category.decode('utf-8'):\n",
    "                        idx.append(i); break;\n",
    "            self.classwiseidx.append(idx)\n",
    "\n",
    "\n",
    "    def load_data(self, n_similar=3, is_training=True):\n",
    "        if is_training==True:\n",
    "            features = []\n",
    "            labels = []\n",
    "            idx = []\n",
    "\n",
    "            # Load similar pairs\n",
    "            rand_classid = np.random.choice(len(self.classwiseidx), size=n_similar)\n",
    "            for rid in rand_classid:\n",
    "                rand_sampleid = np.random.choice(len(self.classwiseidx[rid]), size=2)\n",
    "                idx.append(self.classwiseidx[rid][rand_sampleid[0]])\n",
    "                idx.append(self.classwiseidx[rid][rand_sampleid[1]])\n",
    "\n",
    "            # Load rest pairs\n",
    "            rand_sampleid = np.random.choice(len(self.trainidx), size=self.batch_size-2*n_similar)\n",
    "            for r in rand_sampleid:\n",
    "                idx.append(self.trainidx[r])\n",
    "          \n",
    "            return np.array([util.process_feat(self.features[i], self.t_max) for i in idx]), np.array([self.labels_multihot[i] for i in idx])\n",
    "\n",
    "        else:\n",
    "            labs = self.labels_multihot[self.testidx[self.currenttestidx]]\n",
    "            feat = self.features[self.testidx[self.currenttestidx]]\n",
    "\n",
    "            if self.currenttestidx == len(self.testidx)-1:\n",
    "                done = True; self.currenttestidx = 0\n",
    "            else:\n",
    "                done = False; self.currenttestidx += 1\n",
    "         \n",
    "            return np.array(feat), np.array(labs), done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.options\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.lr = 0.0001\n",
    "        self.dataset_name = 'Thumos14reduced'\n",
    "        self.num_class = 20\n",
    "        self.feature_size = 2048\n",
    "        self.batch_size = 24\n",
    "        self.max_seqlen = 750\n",
    "        self.model_name = 'weakloc'\n",
    "        self.pretrained_ckpt = None\n",
    "        self.max_iter = 50000\n",
    "        self.num_similar = 3\n",
    "        self.checkpoint_path = './checkpoint/'\n",
    "        self.annotation_path = './annotations/'\n",
    "        self.I3D_path = './I3D_features/'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Module\n",
    "# Test\n",
    "\n",
    "class Attention_Module(nn.Module):\n",
    "    \"\"\"Args:\n",
    "    feature_dim: dimension of the feature from I3D model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention_Module, self).__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.fc0 = nn.Linear(feature_dim, 1024)\n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc3 = nn.Linear(2048, 20)\n",
    "\n",
    "    def forward(self, features_list):\n",
    "        \"\"\"Build the attention module.\n",
    "\n",
    "        Args:\n",
    "        features_list: (batch_size, num_frame, feat_depth)\n",
    "\n",
    "        Returns:\n",
    "        The attention weights, weigted features\n",
    "        \"\"\"\n",
    "        \n",
    "        attention_weights = []\n",
    "        weighted_features = []\n",
    "        \n",
    "        # Iterate through batch\n",
    "        for idx, video_features in enumerate(features_list):\n",
    "                        \n",
    "            # Trunk feature into real length\n",
    "            seq_len = (torch.abs(video_features).max(dim=1)[0] > 0).sum().tolist()\n",
    "            video_features = video_features[: seq_len, :]\n",
    "            \n",
    "            # Iterate through video segments\n",
    "            output = self.sigmoid(self.fc2(self.relu(self.fc1(self.relu(self.fc0(video_features))))))\n",
    "\n",
    "            # Temporal Pool\n",
    "            weighted_pooling = (output*video_features).sum(0)/video_features.shape[0]\n",
    "            \n",
    "            # Save weights/features\n",
    "            output = output.reshape(output.shape[0])\n",
    "            attention_weights.append(output)\n",
    "            weighted_features.append(weighted_pooling)\n",
    "            \n",
    "        # Reshape to tensor\n",
    "        weighted_features = torch.stack(weighted_features)\n",
    "#         attention_weights = torch.stack(attention_weights)\n",
    "        \n",
    "        predict = self.sigmoid(self.fc3(weighted_features))\n",
    "        \n",
    "        return attention_weights, weighted_features, predict\n",
    "\n",
    "\n",
    "def l1_norm(inputs):\n",
    "    norm_value = torch.zeros(len(inputs)).cuda()\n",
    "    for idx, vector in enumerate(inputs):\n",
    "        norm_value[idx] = torch.norm(torch.abs(vector), p=1)\n",
    "    return norm_value.sum()\n",
    "\n",
    "\n",
    "def multi_to_single(labels):\n",
    "    l = []\n",
    "    for lab in labels:\n",
    "        l.append(lab.tolist().index(1))\n",
    "    return torch.from_numpy(np.asarray(l)).cuda()\n",
    "\n",
    "\n",
    "# T-cam function\n",
    "def t_cam(model, features, segment_len, cls_num=20):\n",
    "    \"\"\"\n",
    "        return (segments, class_num)\n",
    "    \"\"\"\n",
    "    weights = torch.zeros(segment_len, cls_num)\n",
    "    for i in range(segment_len):\n",
    "        for j in range(cls_num):\n",
    "            weights[i, j] = (features[i]*model.fc3.weight[j]).sum()\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    }
   ],
   "source": [
    "model = Attention_Module(feature_dim=2048)\n",
    "checkpoint = torch.load('./checkpoint/model_epoch_15000_bce.pth')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.eval()\n",
    "model.cuda()\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboard_logger import log_value\n",
    "import utils\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from utils.classificationMAP import getClassificationMAP as cmAP\n",
    "from utils.detectionMAP import getDetectionMAP as dmAP\n",
    "import scipy.io as sio\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def test(itr, dataset, args, model, logger, device):\n",
    "    \n",
    "    done = False\n",
    "    instance_logits_stack = []\n",
    "    element_logits_stack = []\n",
    "    labels_stack = []\n",
    "    while not done:\n",
    "        if dataset.currenttestidx % 100 ==0:\n",
    "            print('Testing test data point %d of %d' %(dataset.currenttestidx, len(dataset.testidx)))\n",
    "\n",
    "        features, labels, done = dataset.load_data(is_training=False)\n",
    "        features = torch.from_numpy(features).float().to(device)\n",
    "        \n",
    "        # Computing T-cam weights\n",
    "        t_cam_weights = t_cam(model, features, len(features))\n",
    "        t_cam_weights = t_cam_weights.cpu().data.numpy()\n",
    "\n",
    "        element_logits_stack.append(t_cam_weights)\n",
    "        labels_stack.append(labels)\n",
    "#         print(dataset.currenttestidx)\n",
    "\n",
    "    labels_stack = np.array(labels_stack)\n",
    "#     return element_logits_stack, labels_stack\n",
    "\n",
    "    dmap, iou = dmAP(element_logits_stack, dataset.path_to_annotations)\n",
    "    \n",
    "    if args.dataset_name == 'Thumos14':\n",
    "        test_set = sio.loadmat('test_set_meta.mat')['test_videos'][0]\n",
    "        for i in range(np.shape(labels_stack)[0]):\n",
    "            if test_set[i]['background_video'] == 'YES':\n",
    "                labels_stack[i, :] = np.zeros_like(labels_stack[i,:])\n",
    "\n",
    "    print('Detection map @ %f = %f' %(iou[0], dmap[0]))\n",
    "    print('Detection map @ %f = %f' %(iou[1], dmap[1]))\n",
    "    print('Detection map @ %f = %f' %(iou[2], dmap[2]))\n",
    "    print('Detection map @ %f = %f' %(iou[3], dmap[3]))\n",
    "    print('Detection map @ %f = %f' %(iou[4], dmap[4]))\n",
    "        \n",
    "#     logger.log_value('Test Classification mAP', cmap, itr)\n",
    "#     for item in list(zip(dmap,iou)):\n",
    "#     \tlogger.log_value('Test Detection mAP @ IoU = ' + str(item[1]), item[0], itr)\n",
    "    return element_logits_stack, labels_stack\n",
    "\n",
    "#     utils.write_to_file(args.dataset_name, dmap, cmap, itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing test data point 0 of 212\n",
      "Testing test data point 100 of 212\n",
      "Testing test data point 200 of 212\n",
      "Testing for IoU 0.100000\n",
      "Testing for IoU 0.200000\n",
      "Testing for IoU 0.300000\n",
      "Testing for IoU 0.400000\n",
      "Testing for IoU 0.500000\n",
      "Detection map @ 0.100000 = 37.418052\n",
      "Detection map @ 0.200000 = 31.486944\n",
      "Detection map @ 0.300000 = 21.474831\n",
      "Detection map @ 0.400000 = 14.364203\n",
      "Detection map @ 0.500000 = 9.033523\n"
     ]
    }
   ],
   "source": [
    "from tensorboard_logger import Logger\n",
    "logger = Logger('./logs/' + 'Testing')\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "element_logits_stack, labels_stack = test(0, dataset, args, model, logger, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for IoU 0.100000\n",
      "Testing for IoU 0.200000\n",
      "Testing for IoU 0.300000\n",
      "Testing for IoU 0.400000\n",
      "Testing for IoU 0.500000\n"
     ]
    }
   ],
   "source": [
    "dmap, iou = dmAP(element_logits_stack, dataset.path_to_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detection map @ 0.100000 = 37.418052\n",
      "Detection map @ 0.200000 = 31.486944\n",
      "Detection map @ 0.300000 = 21.474831\n",
      "Detection map @ 0.400000 = 14.364203\n",
      "Detection map @ 0.500000 = 9.033523\n"
     ]
    }
   ],
   "source": [
    "    if args.dataset_name == 'Thumos14':\n",
    "        test_set = sio.loadmat('test_set_meta.mat')['test_videos'][0]\n",
    "        for i in range(np.shape(labels_stack)[0]):\n",
    "            if test_set[i]['background_video'] == 'YES':\n",
    "                labels_stack[i, :] = np.zeros_like(labels_stack[i,:])\n",
    "\n",
    "    print('Detection map @ %f = %f' %(iou[0], dmap[0]))\n",
    "    print('Detection map @ %f = %f' %(iou[1], dmap[1]))\n",
    "    print('Detection map @ %f = %f' %(iou[2], dmap[2]))\n",
    "    print('Detection map @ %f = %f' %(iou[3], dmap[3]))\n",
    "    print('Detection map @ %f = %f' %(iou[4], dmap[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
