{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Gumbel Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tau range between [0.1-1], the smaller the closer to softmax\n",
    "self.tau = 0.3 \n",
    "def gumbelSoftmax(x):        \n",
    "    gumbelNoiseArray = -torch.log(-torch.log((torch.rand_like(x)+0.001) / 1.002))        \n",
    "    softmaxOutput = (F.softmax(x,1)+0.001) / 1.002        \n",
    "    noisySoftmaxOutput = (torch.log(softmaxOutput)+gumbelNoiseArray) / 0.3\n",
    "#     noisySoftmaxOutput = (torch.log(softmaxOutput)+gumbelNoiseArray) / self.tau\n",
    "    return F.softmax(noisySoftmaxOutput, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2a88d11080>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncBdCCBv6/1WWo/cutU/bAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2a88e2fcc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = torch.rand(1, 10)\n",
    "plt.plot(x.numpy()[0])\n",
    "plt.plot(gumbelSoftmax(x).numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Top-Down Attention Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load arguments\n",
    "import os\n",
    "import re\n",
    "import utils\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "import pickle\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utils.options\n",
    "import torch.nn as nn\n",
    "import scipy.io as sio\n",
    "import torch.nn.functional as F\n",
    "from nltk.stem import PorterStemmer\n",
    "from torch.autograd import Variable\n",
    "from tensorboardX import SummaryWriter\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from utils.video_dataset import Dataset\n",
    "from tensorboard_logger import log_value\n",
    "from utils.detectionMAP import getDetectionMAP as dmAP\n",
    "from utils.classificationMAP import getClassificationMAP as cmAP\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.lr = 0.0001\n",
    "        self.dataset_name = 'Thumos14reduced'\n",
    "        self.num_class = 20\n",
    "        self.feature_size = 2048\n",
    "        self.batch_size = 24\n",
    "        self.max_seqlen = 750\n",
    "        self.model_name = 'weakloc'\n",
    "        self.pretrained_ckpt = None\n",
    "        self.max_iter = 50000\n",
    "        self.num_similar = 3\n",
    "        self.checkpoint_path = './checkpoint/'\n",
    "        self.annotation_path = './annotations/'\n",
    "        self.I3D_path = './I3D_features/'\n",
    "\n",
    "\n",
    "# Category to sentence\n",
    "class_name = {0: [\"baseball pitch\", \"throw a baseball\", \"baseball throw\"],\n",
    "             1: [\"basketball dunk\", \"dunk a basketball\", \"slam dunk basketball\"],\n",
    "             2: [\"billiards\"],\n",
    "             3: [\"clean and jerk\", \"weight lifting movement\"],\n",
    "             4: [\"cliff diving\", \"high diving\", \"diving\"],\n",
    "             5: [\"cricket shot\" ],\n",
    "             6: [\"cricket bowling\", \"cricket movement\", \"bowl cricket\"],\n",
    "             7: [\"diving\", \"jumping into water\", \"falling into water\"],\n",
    "             8: [\"frisbee catch\", \"catch frisbee\"],\n",
    "             9: [\"golf swing\", \"golf stroke\"],\n",
    "            10: [\"hammer throw\", \"throw a hammer\"],\n",
    "            11: [\"high jump\"],\n",
    "            12: [\"javelin throw\", \"throw a spear\"],\n",
    "            13: [\"long jump\", \"jump contest\"],\n",
    "            14: [\"pole vault\", \"a person uses a long flexible pole to jump over a bar\"],\n",
    "            15: [\"shot put\"],\n",
    "            16: [\"soccer penalty\"],\n",
    "            17: [\"tennis swing\"],\n",
    "            18: [\"throw discus\", \"discus\"],\n",
    "            19: [\"volleyball spiking\", \"volleyball\", ]}\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Load the dataset\n",
    "dataset = Dataset(args)\n",
    "\n",
    "# Word Embedding Loading from GLOVE\n",
    "path_to_glove = './checkpoint/glove.840B.300d.pkl'\n",
    "\n",
    "with open(path_to_glove, \"rb\") as input_file:\n",
    "    glove_model = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking all laebls to language format\n",
    "def label_to_text(labels):\n",
    "    text_list = []\n",
    "    for label in labels:\n",
    "        idxs = [idx for idx, val in enumerate(label) if val==1]\n",
    "        text = ''\n",
    "        for idx in idxs:\n",
    "            text += ' ' + random.choice(class_name[idx])\n",
    "        text_list.append(text)\n",
    "    return text_list\n",
    "\n",
    "\n",
    "# L1 normalization for weights regularization\n",
    "def l1_norm(inputs):\n",
    "    norm_value = torch.zeros(len(inputs)).cuda()\n",
    "    for idx, vector in enumerate(inputs):\n",
    "        norm_value[idx] = torch.norm(torch.abs(vector), p=1)\n",
    "    return norm_value.sum()\n",
    "\n",
    "\n",
    "# Sample single label from multi ground truth\n",
    "def multi_to_single(labels):\n",
    "    l = []\n",
    "    for lab in labels:\n",
    "        l.append(lab.tolist().index(1))\n",
    "    return torch.from_numpy(np.asarray(l)).cuda()\n",
    "\n",
    "\n",
    "# T-cam with specific label\n",
    "def temporal_proposals(weights, features, clses):\n",
    "    \"\"\"\n",
    "        Return the binary temporal proposals based on T-CAM.\n",
    "        Input: \n",
    "            features:    (batch, # of segments, # of dim)\n",
    "            cls:         (batch, class_label)\n",
    "        Return:    [batch, # of untrimmed segments(binary)]\n",
    "    \"\"\"\n",
    "    proposals = []\n",
    "    # Iterate through batch\n",
    "    for idx, video_feature in enumerate(features):\n",
    "        # Chunk video feature to real length\n",
    "            seq_len = (torch.abs(video_feature).max(dim=1)[0] > 0).sum().tolist()\n",
    "            video_feature = video_feature[: seq_len, :]\n",
    "            scores = np.zeros(seq_len)\n",
    "            for seg_id, feature in enumerate(video_feature):\n",
    "                for cls in clses[idx]:\n",
    "                    score = (feature*weights[cls]).sum()\n",
    "                    scores[seg_id] += score\n",
    "            # Labeling segments with scores larger than threshold\n",
    "            threshold = (np.max(scores) - (np.max(scores) - np.min(scores))*0.5)\n",
    "            proposals.append([1 if s > threshold else 0 for idx, s in enumerate(scores)])\n",
    "    return proposals\n",
    "\n",
    "\n",
    "def temp2(attention_weights, labels):\n",
    "    weights = torch.zeros(len(attention_weights), 20)\n",
    "    idx_list = [i for i, val in enumerate(labels) if val == 1.0]\n",
    "    for idx, val in enumerate(idx_list):\n",
    "        weights[:, val] = attention_weights\n",
    "    return weights\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class Model(nn.Module):\n",
    "    \"\"\"Args:\n",
    "        Video I3D features and raw natural text.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, time_steps, output_dim=100, \n",
    "                 num_layers=2, path_to_glove = './checkpoint/glove.840B.300d.pkl'):\n",
    "        super(Model, self).__init__()\n",
    "        \"\"\"\n",
    "            Create Duo Strean Model.\n",
    "        \"\"\"\n",
    "        self.textual_model = Language_encoder(lstm_input_size, hidden_dim, batch_size=batch_size, \n",
    "                                 time_steps=8, output_dim=output_dim, num_layers=num_layers, path_to_glove\n",
    "                                              =path_to_glove)\n",
    "        self.visual_model = Visual_model()\n",
    "    \n",
    "    def forward(self, visual_feature, text, t_proposals=None, test=False):\n",
    "        \"\"\"\n",
    "            pos/neg_feature: (batch, 2048).       \n",
    "                mean_pooled representations.\n",
    "        \"\"\"\n",
    "        textual_feature = self.textual_model(text)\n",
    "        attention_weights, visual_feature, pos_feature, neg_feature, test_features = \\\n",
    "            self.visual_model(visual_feature, text, t_proposals=t_proposals, test=test)\n",
    "\n",
    "        return attention_weights, visual_feature, textual_feature, pos_feature, neg_feature, test_features\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Build up the language model and LSTM\n",
    "class Language_encoder(nn.Module):\n",
    "    \"\"\"Args:\n",
    "        Natural Language Text.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, time_steps, output_dim=100, \n",
    "                 num_layers=2, path_to_glove = './checkpoint/glove.840B.300d.pkl'):\n",
    "        super(Language_encoder, self).__init__()\n",
    "        \"\"\"\n",
    "            Load GLOVE pre-trained model first.\n",
    "        \"\"\"\n",
    "        with open(path_to_glove, \"rb\") as input_file:\n",
    "            self.glove = pickle.load(input_file)\n",
    "        self.wordnet_lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_layers = num_layers\n",
    "        self.time_steps = time_steps\n",
    "        \n",
    "        # Define the LSTM/fc module\n",
    "        self.lstm = nn.LSTM(self.input_dim, self.hidden_dim, self.num_layers)\n",
    "        self.linear = nn.Linear(self.hidden_dim, output_dim)\n",
    "\n",
    "    def language_preprocess(self, input_str):\n",
    "        # convert to lowercase\n",
    "        input_str = input_str.lower()\n",
    "        # remove numbers\n",
    "        input_str = re.sub(r'\\d+', '', input_str)\n",
    "        # remove punctuation\n",
    "        input_str = re.sub(r'[^\\w\\s]','',input_str)\n",
    "        # remove whitespaces\n",
    "        input_str = input_str.strip()\n",
    "        # remove stop words\n",
    "        stop_words = set(ENGLISH_STOP_WORDS)\n",
    "        tokens = word_tokenize(input_str)\n",
    "        words = [i for i in tokens if not i in stop_words]\n",
    "        # stemming the words\n",
    "        words = [self.wordnet_lemmatizer.lemmatize(word) for word in words]\n",
    "        return words\n",
    "    \n",
    "    def zero_pad_feature(self, word_dict):\n",
    "        # transform to tensor input, and zero padding the extra feature\n",
    "        input_seq = torch.zeros(self.time_steps, len(word_dict), lstm_input_size).cuda()\n",
    "        \n",
    "        for i in range(len(word_dict)):\n",
    "            for j in range(self.time_steps):\n",
    "                if j < len(word_dict[i]):\n",
    "                    # word2vec extracting\n",
    "                    input_seq[j, i, :] = torch.from_numpy(glove_model[word_dict[i][j]]).cuda()\n",
    "        return input_seq\n",
    "    \n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "            Text will be pre-processed first, and zero-padded for LSTM input.\n",
    "        \"\"\"\n",
    "        word_dict = []\n",
    "        for lines in text:\n",
    "            word_dict.append(self.language_preprocess(lines))\n",
    "        input_seq = self.zero_pad_feature(word_dict)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        last_output = lstm_out[-1]\n",
    "        output = self.linear(last_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "    \n",
    "class Visual_model(nn.Module):\n",
    "    \"\"\"Args:\n",
    "    feature_dim: dimension of the feature from I3D model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim=2048):\n",
    "        super(Visual_model, self).__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.fc0 = nn.Linear(feature_dim, 1024)\n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc_lr = nn.Linear(2048, 100)\n",
    "        self.language_encoder = Language_encoder(300, 300, batch_size=1, time_steps=8, output_dim=256, num_layers=2, \n",
    "                                            path_to_glove=path_to_glove)\n",
    "        \n",
    "        # Bilinear Module: input: (# seg * 2048d), (1 * 300d); output: (# seg * 256)\n",
    "        self.bilinear_pooling = nn.Bilinear(256, 100, 256)\n",
    "\n",
    "    def forward(self, features_list, text_list, t_proposals=None, test=False):\n",
    "        \"\"\"Build the attention module.\n",
    "\n",
    "        Args:\n",
    "        features_list: (batch_size, num_frame, feat_depth)\n",
    "        t_proposals:    temporal proposals generated from T-CAM, for boostrapping LSTM training.\n",
    "        language_vector: top-down signal, (batch_size, feat_dim)\n",
    "\n",
    "        Returns:\n",
    "        The attention weights, weigted features\n",
    "        \"\"\"\n",
    "        # Test_features are returning tensors (batch, length, feat_dim)\n",
    "        attention_weights = []\n",
    "        weighted_features = []\n",
    "        pos_features = []\n",
    "        neg_features = []\n",
    "        test_features = []\n",
    "        text_features = self.language_encoder(text_list)\n",
    "        \n",
    "        # Iterate through batch since length of each video segment varies\n",
    "        for idx, video_features in enumerate(features_list):\n",
    "            # Trunk feature into real length\n",
    "            seq_len = (torch.abs(video_features).max(dim=1)[0] > 0).sum().tolist()\n",
    "            video_features = video_features[: seq_len, :]\n",
    "\n",
    "            # Expand the size of language size\n",
    "            language_feat = text_features[idx]\n",
    "            language_feat = self.sigmoid(language_feat.expand(video_features.shape[0], language_feat.shape[-1]))\n",
    "\n",
    "            # Iterate through video segments\n",
    "#             bilinear = self.relu(self.bilinear_pooling(self.relu(self.fc1(self.relu(self.fc0(video_features)))), language_feat))\n",
    "            bilinear = self.relu(self.fc1(self.relu(self.fc0(video_features)))*language_feat)\n",
    "            output = self.sigmoid(self.fc2(bilinear))\n",
    "\n",
    "            # Temporal Pool\n",
    "            weighted_pooling = (output*video_features).sum(0)/video_features.shape[0]\n",
    "            \n",
    "            # If testing, feed weighted video_features (no mean pool) to fc_lr\n",
    "            if test:\n",
    "                test_features.append(self.fc_lr((output*video_features)))\n",
    "            \n",
    "            # Save weights/features\n",
    "            output = output.reshape(output.shape[0])\n",
    "            attention_weights.append(output)\n",
    "            weighted_features.append(weighted_pooling)\n",
    "\n",
    "            # Pool pos/neg segments features from T-CAM proposals\n",
    "            if t_proposals is not None:\n",
    "                pos_list = [index for index, l in enumerate(t_proposals[idx]) if l == 1.]\n",
    "                neg_list = [index for index, l in enumerate(t_proposals[idx]) if l == 0.]\n",
    "                \n",
    "                pos_feature = torch.stack([feat for index, feat in enumerate(video_features) \n",
    "                                           if index in pos_list]).sum(0)/len(pos_list)\n",
    "                neg_feature = torch.stack([feat for index, feat in enumerate(video_features) \n",
    "                                           if index in neg_list]).sum(0)/len(neg_list)\n",
    "                \n",
    "                pos_features.append(pos_feature)\n",
    "                neg_features.append(neg_feature)\n",
    "                \n",
    "        # Reshape to tensor\n",
    "        weighted_features = torch.stack(weighted_features)\n",
    "\n",
    "        pos_features = torch.stack(pos_features)\n",
    "        neg_features = torch.stack(neg_features)\n",
    "        \n",
    "        # Feed temporal features to fc regression layer\n",
    "        # output: aggregated visual representations\n",
    "        output = self.fc_lr(weighted_features)\n",
    "        \n",
    "        if test:\n",
    "            return attention_weights, output, None, None, test_features\n",
    "        else:\n",
    "            # training without t-cam bootstrapping\n",
    "            if t_proposals is None:\n",
    "                return attention_weights, output, None, None, None\n",
    "            # training with t-cam bootstrapping\n",
    "            else:\n",
    "                pos_features = self.fc_lr(pos_features)\n",
    "                neg_features = self.fc_lr(neg_features)\n",
    "                return attention_weights, output, pos_features, neg_features, None\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model created\n"
     ]
    }
   ],
   "source": [
    "# Duo Model testing\n",
    "lstm_input_size = 300\n",
    "hidden_dim = 300\n",
    "batch_size = 1\n",
    "output_dim = 100\n",
    "num_layers = 2\n",
    "\n",
    "model = Model(lstm_input_size, hidden_dim, batch_size=batch_size, \n",
    "                                 time_steps=8, output_dim=output_dim, num_layers=num_layers)\n",
    "\n",
    "# Load pre-trained classification network for bootstrapping\n",
    "checkpoint = torch.load('./checkpoint/temporal_cls_epoch_15000_bceloss.pth')\n",
    "pretrained_dict = checkpoint['state_dict']\n",
    "\n",
    "fc_weight = pretrained_dict['fc3.weight']\n",
    "\n",
    "# filter out unnecessary keys and load valid params\n",
    "model_dict = model.visual_model.state_dict()\n",
    "pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "model_dict.update(pretrained_dict) \n",
    "model.visual_model.load_state_dict(model_dict)\n",
    "\n",
    "\n",
    "model.cuda()\n",
    "model.textual_model.cuda()\n",
    "model.visual_model.cuda()\n",
    "print('model created')\n",
    "\n",
    "# Store the fc weights for t-cam prediction\n",
    "# args.weights = model.visual_model.fc3.weight.detach().cpu()\n",
    "args.weights = fc_weight.cpu()\n",
    "\n",
    "# Loss defined here\n",
    "marginrankingloss = nn.MarginRankingLoss(0.1)\n",
    "\n",
    "def euclidean_distance(v1, v2, dim=None):\n",
    "    if dim is not None: \n",
    "        return torch.mean((v1-v2)**2, dim)\n",
    "    else:\n",
    "        return torch.mean((v1-v2)**2)\n",
    "\n",
    "# Optimizer\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.max_iter):\n",
    "    # Randomly extract 10 video clips' I3D feature\n",
    "    features, labels = dataset.load_data(n_similar=args.num_similar)\n",
    "\n",
    "    # Features are alingned in 750 frames all the same, now trunk it into max length\n",
    "    seq_len = np.sum(np.max(np.abs(features), axis=2) > 0, axis=1)\n",
    "    features = features[:,:np.max(seq_len),:]\n",
    "\n",
    "    # Convert to CUDA tensor\n",
    "    features = torch.from_numpy(features).float().to('cuda')\n",
    "    labels = torch.from_numpy(labels).float().to('cuda')\n",
    "\n",
    "    # Generate texts from categories\n",
    "    text_list = label_to_text(labels)\n",
    "\n",
    "    # Generate Pos/Neg temporal segment mask for contrastive loss\n",
    "    clses = [[idx for idx, cls in enumerate(label) if cls == 1.] for label in labels]\n",
    "    t_proposals = temporal_proposals(args.weights, features.detach().cpu(), clses)\n",
    "\n",
    "    attention_weights, visual_feature, textual_feature, pos_feature, neg_feature, test_features\\\n",
    "        = model(features, text_list, t_proposals)\n",
    "\n",
    "    # Squared Loss, Margin Ranking Loss\n",
    "    pos_distance = euclidean_distance(pos_feature, textual_feature, 1)\n",
    "    neg_distance = euclidean_distance(neg_feature, textual_feature, 1)\n",
    "    target = torch.zeros(visual_feature.shape[0]).cuda()-1\n",
    "\n",
    "    margin_loss = marginrankingloss(pos_distance, neg_distance, target)\n",
    "\n",
    "    loss = euclidean_distance(visual_feature, textual_feature)+0.01*margin_loss+0.000001*l1_norm(attention_weights)\n",
    "\n",
    "\n",
    "    # Back Propogation\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()  \n",
    "    optimizer.step()\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % 5000 == 0 and epoch is not 0:\n",
    "        # Reduce lr \n",
    "        lr /= 2\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "        # Checkpoint structure\n",
    "        model_state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(model_state, os.path.join(args.checkpoint_path, 'model_nlp_epoch_{:03}.pth'.format(epoch)))\n",
    "    \n",
    "\n",
    "    # Print out training loss\n",
    "    loss_value = loss.detach().cpu().tolist()\n",
    "    writer.add_scalar('runs/', loss_value, epoch)\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch:{:03}, Loss: {:02}'.format(epoch, loss_value))\n",
    "        # Display attention weights and loss value\n",
    "        plt.plot(attention_weights[0].tolist(), c='b')\n",
    "        plt.plot(t_proposals[0], c='r')\n",
    "        plt.show()\n",
    "        \n",
    "# export scalar data to JSON for external processing\n",
    "writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
