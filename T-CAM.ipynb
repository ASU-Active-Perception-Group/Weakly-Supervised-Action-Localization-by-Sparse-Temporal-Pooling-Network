{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Thumos 2014 I3D features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import utils.utils as util\n",
    "import time\n",
    "import os\n",
    "\n",
    "class Dataset():\n",
    "    def __init__(self, args):\n",
    "        self.trainidx = []\n",
    "        self.dataset_name = args.dataset_name\n",
    "        self.path_to_annotations = os.path.join(args.annotation_path, args.dataset_name + '-Annotations/')\n",
    "        self.path_to_features = os.path.join(args.I3D_path, self.dataset_name + '-I3D-JOINTFeatures.npy')\n",
    "        self.labels = np.load(self.path_to_annotations + 'labels_all.npy')     # Specific to Thumos14\n",
    "        self.classlist = np.load(self.path_to_annotations + 'classlist.npy')\n",
    "        self.subset = np.load(self.path_to_annotations + 'subset.npy')\n",
    "        self.testidx = []\n",
    "        self.classwiseidx = []\n",
    "        self.train_test_idx()\n",
    "        self.currenttestidx = 0\n",
    "        self.t_max = args.max_seqlen\n",
    "        self.num_class = args.num_class\n",
    "        self.classwise_feature_mapping()\n",
    "        self.batch_size = args.batch_size\n",
    "        self.feature_size = args.feature_size\n",
    "        self.features = np.load(self.path_to_features, encoding='bytes')\n",
    "        self.segments = np.load(self.path_to_annotations + 'segments.npy')\n",
    "        self.labels_multihot = [util.strlist2multihot(labs,self.classlist) for labs in self.labels]\n",
    "\n",
    "\n",
    "    def train_test_idx(self):\n",
    "        for i, s in enumerate(self.subset):\n",
    "            if s.decode('utf-8') == 'validation':   # Specific to Thumos14\n",
    "                self.trainidx.append(i)\n",
    "            else:\n",
    "                self.testidx.append(i)\n",
    "\n",
    "    def classwise_feature_mapping(self):\n",
    "        for category in self.classlist:\n",
    "            idx = []\n",
    "            for i in self.trainidx:\n",
    "                for label in self.labels[i]:\n",
    "                    if label == category.decode('utf-8'):\n",
    "                        idx.append(i); break;\n",
    "            self.classwiseidx.append(idx)\n",
    "\n",
    "\n",
    "    def load_data(self, n_similar=3, is_training=True):\n",
    "        if is_training==True:\n",
    "            features = []\n",
    "            labels = []\n",
    "            idx = []\n",
    "\n",
    "            # Load similar pairs\n",
    "            rand_classid = np.random.choice(len(self.classwiseidx), size=n_similar)\n",
    "            for rid in rand_classid:\n",
    "                rand_sampleid = np.random.choice(len(self.classwiseidx[rid]), size=2)\n",
    "                idx.append(self.classwiseidx[rid][rand_sampleid[0]])\n",
    "                idx.append(self.classwiseidx[rid][rand_sampleid[1]])\n",
    "\n",
    "            # Load rest pairs\n",
    "            rand_sampleid = np.random.choice(len(self.trainidx), size=self.batch_size-2*n_similar)\n",
    "            for r in rand_sampleid:\n",
    "                idx.append(self.trainidx[r])\n",
    "          \n",
    "            return np.array([util.process_feat(self.features[i], self.t_max) for i in idx]), np.array([self.labels_multihot[i] for i in idx])\n",
    "\n",
    "        else:\n",
    "            labs = self.labels_multihot[self.testidx[self.currenttestidx]]\n",
    "            feat = self.features[self.testidx[self.currenttestidx]]\n",
    "\n",
    "            if self.currenttestidx == len(self.testidx)-1:\n",
    "                done = True; self.currenttestidx = 0\n",
    "            else:\n",
    "                done = False; self.currenttestidx += 1\n",
    "         \n",
    "            return np.array(feat), np.array(labs), done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.options\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.lr = 0.0001\n",
    "        self.dataset_name = 'Thumos14reduced'\n",
    "        self.num_class = 20\n",
    "        self.feature_size = 2048\n",
    "        self.batch_size = 24\n",
    "        self.max_seqlen = 750\n",
    "        self.model_name = 'weakloc'\n",
    "        self.pretrained_ckpt = None\n",
    "        self.max_iter = 50000\n",
    "        self.num_similar = 3\n",
    "        self.checkpoint_path = './checkpoint/'\n",
    "        self.annotation_path = './annotations/'\n",
    "        self.I3D_path = './I3D_features/'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Load the dataset\n",
    "thomoas_i3d_dataset = Dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Thumas Dataset\n",
    "\n",
    "# Randomly extract 10 video clips' I3D feature\n",
    "features, labels = thomoas_i3d_dataset.load_data(n_similar=args.num_similar, is_training=True)\n",
    "\n",
    "# Features are alingned in 750 frames all the same, now trunk it into actual max length\n",
    "seq_len = np.sum(np.max(np.abs(features), axis=2) > 0, axis=1)\n",
    "features = features[:,:np.max(seq_len),:]\n",
    "\n",
    "# Convert to CUDA tensor\n",
    "features = torch.from_numpy(features).float().to('cuda')\n",
    "labels = torch.from_numpy(labels).float().to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Pooling Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Module\n",
    "# Test\n",
    "\n",
    "class Attention_Module(nn.Module):\n",
    "    \"\"\"Args:\n",
    "    feature_dim: dimension of the feature from I3D model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim):\n",
    "        super(Attention_Module, self).__init__()\n",
    "        \n",
    "        self.feature_dim = feature_dim\n",
    "        self.fc0 = nn.Linear(feature_dim, 1024)\n",
    "        self.fc1 = nn.Linear(1024, 256)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc3 = nn.Linear(2048, 20)\n",
    "\n",
    "    def forward(self, features_list):\n",
    "        \"\"\"Build the attention module.\n",
    "\n",
    "        Args:\n",
    "        features_list: (batch_size, num_frame, feat_depth)\n",
    "\n",
    "        Returns:\n",
    "        The attention weights, weigted features\n",
    "        \"\"\"\n",
    "        \n",
    "        attention_weights = []\n",
    "        weighted_features = []\n",
    "        \n",
    "        # Iterate through batch\n",
    "        for idx, video_features in enumerate(features_list):\n",
    "                        \n",
    "            # Trunk feature into real length\n",
    "            seq_len = (torch.abs(video_features).max(dim=1)[0] > 0).sum().tolist()\n",
    "            video_features = video_features[: seq_len, :]\n",
    "            \n",
    "            # Iterate through video segments\n",
    "            output = self.sigmoid(self.fc2(self.relu(self.fc1(self.relu(self.fc0(video_features))))))\n",
    "\n",
    "            # Temporal Pool\n",
    "            weighted_pooling = (output*video_features).sum(0)/video_features.shape[0]\n",
    "            \n",
    "            # Save weights/features\n",
    "            output = output.reshape(output.shape[0])\n",
    "            attention_weights.append(output)\n",
    "            weighted_features.append(weighted_pooling)\n",
    "            \n",
    "        # Reshape to tensor\n",
    "        weighted_features = torch.stack(weighted_features)\n",
    "        \n",
    "        predict = self.sigmoid(self.fc3(weighted_features))\n",
    "        \n",
    "        return attention_weights, weighted_features, predict\n",
    "\n",
    "\n",
    "def l1_norm(inputs):\n",
    "    norm_value = torch.zeros(len(inputs)).cuda()\n",
    "    for idx, vector in enumerate(inputs):\n",
    "        norm_value[idx] = torch.norm(torch.abs(vector), p=1)\n",
    "    return norm_value.sum()\n",
    "\n",
    "\n",
    "def multi_to_single(labels):\n",
    "    l = []\n",
    "    for lab in labels:\n",
    "        l.append(lab.tolist().index(1))\n",
    "    return torch.from_numpy(np.asarray(l)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Model loaded\n"
     ]
    }
   ],
   "source": [
    "# Attention Module Testing\n",
    "model = Attention_Module(feature_dim=2048)\n",
    "model.cuda()\n",
    "print('Attention Model loaded')\n",
    "\n",
    "# Optimizer\n",
    "lr = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "# Softmax Loss Define\n",
    "softmax_loss = nn.BCELoss()\n",
    "# margin_loss = nn.MultiLabelSoftMarginLoss()\n",
    "# cross_loss = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in range(args.max_iter):\n",
    "    # Randomly extract 10 video clips' I3D feature\n",
    "    features, labels = thomoas_i3d_dataset.load_data(n_similar=args.num_similar)\n",
    "\n",
    "    # Features are alingned in 750 frames all the same, now trunk it into max length\n",
    "    seq_len = np.sum(np.max(np.abs(features), axis=2) > 0, axis=1)\n",
    "    features = features[:,:np.max(seq_len),:]\n",
    "\n",
    "    # Convert to CUDA tensor\n",
    "    features = torch.from_numpy(features).float().to('cuda')\n",
    "    labels = torch.from_numpy(labels).float().to('cuda')\n",
    "    \n",
    "    # Prediction and Loss\n",
    "    attention_weights, weighted_features, output = model(features)\n",
    "    \n",
    "#     s_loss_value = softmax_loss(output, labels).detach().cpu().tolist()\n",
    "    loss = softmax_loss(output, labels) + 0.00001*l1_norm(attention_weights)\n",
    "#     loss = cross_loss(output, multi_to_single(labels).long())\n",
    "    \n",
    "    # Back Propogation\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward()  \n",
    "    optimizer.step()\n",
    "    \n",
    "    # Testing performances\n",
    "#     if epoch % 100 == 0:\n",
    "        # Load test set and conduct validation\n",
    "        \n",
    "        \n",
    "    # Save checkpoint\n",
    "    if epoch % 5000 == 0 and epoch is not 0:\n",
    "        # Reduce lr \n",
    "        lr /= 2\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "        # Checkpoint structure\n",
    "        model_state = {\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(model_state, os.path.join(args.checkpoint_path, 'model_epoch_{:03}.pth'.format(epoch)))\n",
    "    \n",
    "\n",
    "    # Print out training loss\n",
    "    loss_value = loss.detach().cpu().tolist()\n",
    "    writer.add_scalar('runs/', loss_value, epoch)\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:{:03}, Loss: {:02}'.format(epoch, loss_value))\n",
    "\n",
    "        print(attention_weights[0][0:10].tolist())\n",
    "        plt.plot(output[0].tolist(), linestyle='--', marker='o', color='b')\n",
    "        plt.plot(labels[0].tolist(), linestyle='--', marker='o', color='r')\n",
    "        plt.show()\n",
    "\n",
    "# export scalar data to JSON for external processing\n",
    "writer.export_scalars_to_json(\"./all_scalars.json\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Pre-trained Model for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tensorboard_logger import log_value\n",
    "import utils\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from utils.classificationMAP import getClassificationMAP as cmAP\n",
    "from utils.detectionMAP import getDetectionMAP as dmAP\n",
    "import scipy.io as sio\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def test(itr, dataset, args, model, logger, device):\n",
    "    \n",
    "    done = False\n",
    "    instance_logits_stack = []\n",
    "    element_logits_stack = []\n",
    "    labels_stack = []\n",
    "    while not done:\n",
    "        if dataset.currenttestidx % 100 ==0:\n",
    "            print('Testing test data point %d of %d' %(dataset.currenttestidx, len(dataset.testidx)))\n",
    "\n",
    "        features, labels, done = dataset.load_data(is_training=False)\n",
    "        features = torch.from_numpy(features).float().to(device)\n",
    "        \n",
    "        # Computing T-cam weights\n",
    "#         t_cam_weights = t_cam(model, features, len(features))\n",
    "        attention_weights, weighted_features, predict = model([features])\n",
    "        t_cam_weights = temp2(attention_weights[0], labels)\n",
    "        t_cam_weights = t_cam_weights.cpu().data.numpy()\n",
    "\n",
    "        element_logits_stack.append(t_cam_weights)\n",
    "        labels_stack.append(labels)\n",
    "\n",
    "    labels_stack = np.array(labels_stack)\n",
    "#     return element_logits_stack, labels_stack\n",
    "\n",
    "    dmap, iou = dmAP(element_logits_stack, dataset.path_to_annotations)\n",
    "    \n",
    "    if args.dataset_name == 'Thumos14':\n",
    "        test_set = sio.loadmat('test_set_meta.mat')['test_videos'][0]\n",
    "        for i in range(np.shape(labels_stack)[0]):\n",
    "            if test_set[i]['background_video'] == 'YES':\n",
    "                labels_stack[i, :] = np.zeros_like(labels_stack[i,:])\n",
    "\n",
    "    print('Detection map @ %f = %f' %(iou[0], dmap[0]))\n",
    "    print('Detection map @ %f = %f' %(iou[1], dmap[1]))\n",
    "    print('Detection map @ %f = %f' %(iou[2], dmap[2]))\n",
    "    print('Detection map @ %f = %f' %(iou[3], dmap[3]))\n",
    "    print('Detection map @ %f = %f' %(iou[4], dmap[4]))\n",
    "        \n",
    "#     logger.log_value('Test Classification mAP', cmap, itr)\n",
    "#     for item in list(zip(dmap,iou)):\n",
    "#     \tlogger.log_value('Test Detection mAP @ IoU = ' + str(item[1]), item[0], itr)\n",
    "    return element_logits_stack, labels_stack\n",
    "\n",
    "#     utils.write_to_file(args.dataset_name, dmap, cmap, itr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Attention_Module(feature_dim=2048)\n",
    "checkpoint = torch.load('./checkpoint/temporal_cls_epoch_15000_bceloss.pth')\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "model.eval()\n",
    "model.cuda()\n",
    "print('model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard_logger import Logger\n",
    "logger = Logger('./logs/' + 'Testing')\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "element_logits_stack, labels_stack = test(0, dataset, args, model, logger, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
